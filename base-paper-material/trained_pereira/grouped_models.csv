layers,neurons,models
1,200,['ETM']
1,300,"['glove', 'word2vec']"
1,2400,['skip-thoughts']
2,1024,['lm_1b']
6,512,['t5-small']
7,768,"['distilbert-base-uncased', 'distilroberta-base', 'distilgpt2']"
7,1024,"['xlm-mlm-enfr-1024', 'xlm-clm-enfr-1024']"
12,768,"['xlnet-base-cased', 't5-base']"
13,768,"['bert-base-uncased', 'bert-base-multilingual-cased', 'roberta-base', 'xlm-roberta-base', 'albert-base-v1', 'albert-base-v2', 'openaigpt', 'gpt2']"
13,1024,['xlm-mlm-xnli15-1024']
13,2048,['xlm-mlm-en-2048']
13,4096,"['albert-xxlarge-v1', 'albert-xxlarge-v2']"
17,1280,['xlm-mlm-100-1280']
19,1024,['transfo-xl-wt103']
24,1024,"['t5-large', 't5-3b', 't5-11b']"
25,1024,"['bert-large-uncased', 'bert-large-uncased-whole-word-masking', 'roberta-large', 'xlm-roberta-large', 'xlnet-large-cased', 'albert-large-v1', 'albert-large-v2', 'gpt2-medium']"
25,2048,"['albert-xlarge-v1', 'albert-xlarge-v2']"
37,1280,['gpt2-large']
49,1280,['ctrl']
49,1600,['gpt2-xl']
